{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T22:20:13.838395Z",
     "iopub.status.busy": "2024-10-25T22:20:13.837992Z"
    },
    "papermill": {
     "duration": 9418.719258,
     "end_time": "2024-10-26T01:02:44.085702",
     "exception": false,
     "start_time": "2024-10-25T22:25:45.366444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 144 minutes 38 seconds.\n",
      "Epoch 1, Train loss: 0.0982722080888537\n",
      "Epoch 1, Validation Accuracy: 0.8158333333333333, F1 Score: 0.8628181253879579\n",
      "Best F1 Score improved to 0.8628181253879579, saving model...\n",
      "Model saved after training on the next 30k samples.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Load dataset from jsonl files\n",
    "def load_jsonl_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_jsonl_data('/kaggle/input/mgt-classification/en_train.jsonl')\n",
    "dev_data = load_jsonl_data('/kaggle/input/mgt-classification/en_dev.jsonl')\n",
    "\n",
    "# Combine train and dev data\n",
    "df = pd.concat([train_data, dev_data])\n",
    "\n",
    "# Select the next 30,000 samples (from index 30,001 to 60,000)\n",
    "df_next = df.iloc[30000:60000].copy()\n",
    "\n",
    "# Prepare Dataset class for PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',  \n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Custom collate function to dynamically pad the batches\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    \n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Split the next 30,000 samples into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_next['text'].values, df_next['label'].values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load the previously saved model and tokenizer from the correct path\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('/kaggle/input/phase-1/distilbert_model_phase_1')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('/kaggle/input/phase-1/distilbert_model_phase_1')\n",
    "\n",
    "# Set dataset parameters\n",
    "MAX_LEN = 128  # Reduced max length for faster training\n",
    "BATCH_SIZE = 8  # Smaller batch size\n",
    "accumulation_steps = 4  # Gradient accumulation\n",
    "\n",
    "# Prepare Dataloaders\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Class imbalance handling (optional)\n",
    "class_weights = torch.tensor([0.7, 1.3]).to(model.device)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, epochs=1, learning_rate=2e-5, weight_decay=0.01):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=2, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss_train = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0] / accumulation_steps  # Scale loss for gradient accumulation\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} completed in {epoch_time // 60:.0f} minutes {epoch_time % 60:.0f} seconds.')\n",
    "        print(f'Epoch {epoch+1}, Train loss: {total_loss_train/len(train_loader)}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        predictions, true_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs[0]\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(true_labels, predictions)\n",
    "        val_f1 = f1_score(true_labels, predictions)\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {val_acc}, F1 Score: {val_f1}')\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"Best F1 Score improved to {best_f1}, saving model...\")\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "# Train the model on the next 30k samples\n",
    "model = train_model(model, train_loader, val_loader, epochs=1)\n",
    "\n",
    "# Ensure the save directory exists for the next phase\n",
    "save_directory = '/kaggle/working/distilbert_model_phase_2'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the trained model and tokenizer for the next phase\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(\"Model saved after training on the next 30k samples.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5857351,
     "sourceId": 9601115,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5949519,
     "sourceId": 9723563,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29907,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 9425.996398,
   "end_time": "2024-10-26T01:02:46.705251",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-25T22:25:40.708853",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
